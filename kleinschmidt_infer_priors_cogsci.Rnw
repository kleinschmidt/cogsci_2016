\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage{siunitx}
\usepackage{dcolumn}

\usepackage{subcaption}

\usepackage{tipx}
\newcommand{\ph}[1]{\protect\textipa{/#1/}}
\newcommand{\aph}[1]{\protect\textipa{[#1]}}

\newcommand{\ms}[1]{\SI{#1}{\milli\second}}

\newcommand{\ud}{\mathrm{d}}

\title{What do you expect from an unfamiliar talker?}
 
\author{{\large \bf Dave F. Kleinschmidt$^1$}, and 
   {\large \bf T. Florian Jaeger$^{1,2,3}$} \\
   \texttt{\{dkleinschmidt, fjeager\} $@$ bcs.rochester.edu} \\
  $^1$Department of Brain and Cognitive Sciences, $^2$Department of Computer Science, and $^3$Department of Linguistics, \\
  University of Rochester, Rochester, NY, 14607 USA}


\begin{document}

\maketitle

\begin{abstract}
\textbf{Keywords:} Cognitive Science, Linguistics, Psychology, Language understanding, Learning, Speech recognition
\end{abstract}



<<preamble, echo=FALSE, results='hide', warning=FALSE, message=FALSE>>=

knitr::opts_chunk$set(echo=FALSE,
                      results='hide',
                      cache=TRUE,
                      autodep=TRUE,
                      message=FALSE,
                      warning=FALSE,
                      error=FALSE)

options(scipen = 2, digits = 2)

library(supunsup)

library(dplyr)
library(tidyr)
library(magrittr)
library(stringr)

library(rstan)

data <- supunsup::supunsup_clean %>%
  filter(supCond == 'unsupervised') %>%
  mutate(subjNum = as.numeric(factor(subject)),
         trueCatNum = as.numeric(trueCat),
         respCatNum = as.numeric(respCat))

library(ggplot2)
theme_set(theme_bw())

library(lme4)

@

\section{Introduction}
\label{sec:introduction}

A longstanding problem in speech perception is how listeners manage to cope with
substantial differences in how individual talkers produce speech.    Recent
evidence suggests that one strategy listeners employ is to \emph{rapidly adapt}
to unfamiliar talkers \cite[among
others]{Bertelson2003,Clarke2004,Kraljic2007,Norris2003}.
\citeA{Kleinschmidt2015} showed that such adaptation can be modeled as a form of
statistical inference.  Each talker's particular accent (way of talking) can be
formalized as the distribution of acoustic cues that they produce for each
phonetic category (or other underlying linguistic unit).  When framed this way,
listeners can adapt to an unfamiliar talker by inferring what those cue
distributions look like, based on the cues that are actually produced by the talker.

One insight that this \emph{ideal adapter} framework provides is that rapid
adaptation to an unfamiliar depends just as much on a listener's prior
experience with other talkers as it does on the speech produced by the
unfamiliar talker themself.  A listener's experience with others talkers
provides the starting point for the distributional learning required for
adaptation, or, in Bayesian terms, a \emph{prior distribution} over possible
accents (cue distributions).  More informative prior beliefs can substantially
reduce the amount of direct evidence needed to converge on accurate beliefs
about the current talker's cue distributions.

In this study, we test a critical predcition of this framework.  To the extent
that a listener's prior beliefs are informative, they must take some probability
\emph{away} from unlikely accents.  Confronted by a talker whose accent falls
well outside the range of what they expect based on their previous experience,
the ideal adapter framework predicts that a listener will require more evidence
to adapt, leading to slowed or incomplete adaptation.  There is some limited
evidence that this is the case.  For instance, \citeA{Idemaru2011} found that
listeners have difficulting adapting to a talker who produces anti-correlated
distributions of two cues that are typically positively or un-correlated.
\citeA{Sumner2011} found that listeners had trouble adapting to a talker who
produced a distribution of cues for the /b/ and /p/ sounds that were
substantially lower than a typical talker.  

However, no studies have systematically probed whether and how a listener's
prior expectations contrain rapid adaptation.  To that end, we expose listeners
to a range of different accents, which differ in the cue distributions for /b/
and /p/.  By parametrically manipulating these distributions, we create a range
of accents that are more or less similar to what a typical talker of English
produces.  We additionally sought to test a further predition. To the extent
that a listener's prior expectations do, in fact, constrain their adaptation, it
would be possible to \emph{infer} what the underlying prior belief distributions
are.  This would provide a powerful method for investigating listeners'
subjective prior beliefs that could be applied to other cues, categories, and
even social variables (like gender, native language background, etc.).

\section{Experiment}
\label{sec:experiment}

\subsection{Methods}
\label{sec:methods}

\subsubsection{Subjects}
\label{sec:subjects}

<<participants>>=
n_subj <- data %>% group_by(subject) %>% summarise() %>% tally()

excluded <- supunsup::supunsup_excluded %>%
  filter(supCond == 'unsupervised') %>%
  group_by(subject) %>% 
  summarise() %>% 
  right_join(supunsup::excludes) %>%
  select(subject, exclude80PercentAcc, rank)

n_excluded <- nrow(excluded)
n_subj_repeat <- sum(!is.na(excluded$rank))
n_subj_bad <- sum(!is.na(excluded$exclude80PercentAcc))
n_both <- n_subj_repeat + n_subj_bad - n_excluded 

n_total <- n_subj + n_excluded

@ 

We recruited \Sexpr{n_total} subjects via Amazon's Mechanical Turk, who were
paid \$2.00 for participation, which took about 20 minutes. We excluded subjects
who participated more than once ($n=\Sexpr{n_subj_repeat}$) or whose accuracy at
\ms{0} and \ms{70} VOT---as extrapolated via a logistic GLM---was less than 80\%
correct ($n=\Sexpr{n_subj_bad}$; $n=\Sexpr{n_both}$ for both reasons). After
these exclusions, data from \Sexpr{n_subj} subjects remained for analysis for
analysis.

\subsubsection{Procedure}
\label{sec:procedure}

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figure_manual/beach_peach.png}
  \caption{Example trial display (beach/peach). Listeners first click on the
    green button to play the word, then click on one picture to indicate what
    they heard.}
  \label{fig:beach-peach}
\end{figure}


\begin{figure*}[htb]
\centering

<<input-vs-prior-stats, fig.width=8, fig.height=2, out.width="\\textwidth">>=

## copied from NIPS paper #######################################

## prior parameters from Kronrod et al. (CogSci 2012)
prior_stats <- data.frame(category=factor(c('b', 'p')),
                          mean = c(0, 60),
                          sd = sqrt(c(14, 254)))

exposure_stats <- data %>%
  group_by(bvotCond, category=trueCat) %>%
  summarise(mean=mean(vot), sd=sd(vot))

sd_noise = sqrt(82)

stats_to_lhood <- function(stats, noise_sd=sd_noise) {
  stats %>%
    group_by(category, mean, sd) %>%
    do(data.frame(vot=seq(-30, 90, 0.5))) %>%
    ungroup() %>%
    mutate(lhood = dnorm(vot, mean, sqrt(sd^2 + noise_sd^2))) %>%
    select(-mean, -sd)
}

exposure_lhood <- exposure_stats %>%
  group_by(bvotCond) %>%
  do(stats_to_lhood(., sd_noise))

prior_lhood <- prior_stats %>% stats_to_lhood(sd_noise)

data %>%
  group_by(bvotCond, vot) %>%
  filter(subject == first(subject)) %>%
  tally() %>%
  ggplot(aes(x=vot)) +
  geom_bar(stat='identity', aes(y=n, fill=bvotCond)) +
  geom_line(data=prior_lhood, aes(y=lhood*1600, group=category),
            color="black", linetype=2) +
  geom_text(data=data.frame(bvotCond=-10), x = 10, y = 60,
            label = 'Typical Talker',
            color='black', hjust=0, vjust=0.3, size=3) +
  geom_text(data=data.frame(bvotCond=-10), x = 40, y = 50,
            label = 'Exposure\nTalker',
            color=hcl(h=15, c=100, l=65), hjust=0, vjust=0.8, size=3,
            lineheight=1) + 
  facet_grid(.~bvotCond) +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Frequency') +
  scale_fill_discrete('/b/ mean\nVOT') +
  theme(legend.position='none')

@ 

\caption{Each subject heard one of these five synthetic accents, which differ
  only in the distribution of VOTs of the word-initial stops. Black dashed lines
  show VOT distributions from a hypothetical typical talker (as estimated by
  \citeNP{Kronrod2012}). Note that the 0 and 10ms shifted accents are reasonably
  close to this typical talker, while the $-10$, 20, and 30ms shifted accents
  deviate substantially.}
\label{fig:vot-dists}

\end{figure*}

Our distributional learning procedure is described in \citeA{Kleinschmidt2015a},
and is based on \citeA{Clayards2008}. On each trial, two response option images
appeard, which corresponded to one of three /b/-/p/ minimal pairs (beach/peach,
bees/peas, or beak/peak). Subjects then clicked on a central button which played
the corresponding minimal pair word, and then clicked on the picture to indicate
whether they heard the /b/ or /p/ member of the minimal pair.  Subjects
performed 222 of these trials.

Each trial's word was synthesized with a voice onset time (VOT) that was
randomly drawn from a bimodal distribution, with low and high VOT clusters
implicitly corresponding to /b/ and /p/, respectively.  This distribution
defined the \emph{accent} that the subject heard, and each subject was
pseudorandomly assigned to one of five accent conditions
(Figure~\ref{fig:vot-dists}).

\subsection{Results and Discussion}
\label{sec:results}

<<regression, eval=FALSE>>=
data_lmer <- supunsup::mutate_for_lmer(data)

## double check that contrasts are helmert coded...
contrasts(data_lmer$bvotCond)

## basically anything more than this doesn't converge...
mod <- glmer(respP ~ vot_rel.s * trial.s + bvotCond + (1 | subject),
             data_lmer, family='binomial')

@ 

\begin{figure*}[htb]
  \centering

<<class-curves, fig.height=2, fig.width=8, out.width="\\textwidth">>=
## generate predicted classification functions assuming Bayes-optimal classifier
## + noise

lhood_to_classification <- function(lhood) {
  lhood %>%
    spread(category, lhood) %>%
    mutate(prob_p = p / (p+b))
}

perfect_learning <- exposure_stats %>%
  group_by(bvotCond) %>%
  do(stats_to_lhood(.)) %>%
  lhood_to_classification

no_learning <- prior_stats %>%
  stats_to_lhood %>%
  lhood_to_classification

prior_bound <- no_learning %>%
  arrange(abs(prob_p - 0.5)) %>%
  filter(row_number() ==1) %$%
  vot


boundaries <- data %>%
  group_by(bvotCond, subject) %>%
  do({ glm(respP ~ vot, family='binomial', data=.) %>%
         broom::tidy() %>%
         select(term, estimate)
  }) %>%
  ungroup() %>%
  spread(term, estimate) %>%
  mutate(boundary = -`(Intercept)` / vot,
         ideal_boundary = as.numeric(as.character(bvotCond)) + 20,
         prior_boundary = prior_bound,
         prop_shift = (boundary-prior_boundary)/(ideal_boundary-prior_boundary))

boundary_summary <- boundaries %>%
  group_by(bvotCond) %>%
  summarise(median_shift_perc = round(100*median(prop_shift)),
            shift_text = paste(median_shift_perc, '%', sep='')) %>%
  filter(bvotCond != 0)                 # basically no shift possible

ggplot(data, aes(x=vot, y=respP, color=bvotCond)) +
  geom_line(aes(group=subject), stat='smooth', method='glm', 
            method.args=list(family='binomial'), alpha=0.2) +
  facet_grid(.~bvotCond) +
  geom_line(data=perfect_learning, aes(y=prob_p), group=1, linetype=2, size=1) +
  geom_line(data=no_learning, aes(y=prob_p), group=1, linetype=2, color='black') +
  geom_text(data=data.frame(bvotCond=-10),
            x = 30, y = 0, label = 'Typical\ntalker',
            size = 3.5, hjust=0, vjust = 0, color='black',
            lineheight=1) + 
  geom_text(data=data.frame(bvotCond=-10),
            x = 12, y = 1, label = 'Expo-\nsure',
            size = 3.5, hjust=1, vjust=1, color=hcl(15, c=100, l=65),
            lineheight=1, fontface='bold') + 
  geom_text(data=data.frame(bvotCond=-10),
            x = 90, y = 0.75, label = 'Actual\nlisteners',
            size = 3.5, hjust=1, vjust=1, color=hcl(15, c=100, l=65),
            lineheight=1) + 
  ## geom_text(data=boundary_summary, aes(x=75, y=0.1, label=shift_text), color='black') + 
  theme(legend.position='none') +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Probability /p/ response') + 
  scale_color_discrete('/b/ mean\nVOT')


@ 
  
\caption{Listeners' responses, smoothed with logistic functions (thin lines),
  compared with the classification functions expected based on a typical talker
  (no learning; dashed black lines) and complete adaptation to the exposure
  distributions (thick dashed colored lines). Listeners' actual category
  boundaries lie between the typical talker and exposure talker boundaries (see
  Table~\ref{tab:boundary-shift}).}
\label{fig:class-curves}
\end{figure*}


<<shift-by-third, eval=FALSE>>=

## Do boundary shift analysis by thirds
boundaries_by_third <-
  data %>%
  mutate(thirds = ntile(trial, 3)) %>%
  group_by(bvotCond, subject, thirds) %>%
  do({ glm(respP ~ vot, family='binomial', data=.) %>%
         broom::tidy() %>%
         select(term, estimate)
  }) %>%
  ungroup() %>%
  spread(term, estimate) %>%
  mutate(boundary = -`(Intercept)` / vot,
         ideal_boundary = as.numeric(as.character(bvotCond)) + 20,
         prior_boundary = prior_bound,
         prop_shift = (boundary-prior_boundary)/(ideal_boundary-prior_boundary)) %>%
  group_by(bvotCond, thirds) %>% 
  do(daver::boot_ci(., function(d,i) {mean(d$prop_shift[i])}))
 
ggplot(boundaries_by_third, 
       aes(x=thirds, y=observed, ymin=ci_lo, ymax=ci_high, 
           color=bvotCond, fill=bvotCond)) + 
  geom_ribbon(alpha=0.2, size=0) + 
  geom_line() + 
  facet_grid(.~bvotCond)

@ 


\begin{table}[htb]
  \centering
<<boundary-shift-table, results='asis'>>=

## bootstrapped boundary summary
boundaries %>% 
  group_by(bvotCond) %>% 
  do(daver::boot_ci(., function(d,i) {mean(d$prop_shift[i])})) %>%
  mutate(observed = round(observed * 100),
         ci_lo = round(ci_lo * 100),
         ci_high = round(ci_high * 100)) %>%
  ungroup() %>%
  transmute(`/b/ mean VOT` = bvotCond,
            `\\% boundary shift` = observed,
            `95\\% CI (bootstrapped)` = sprintf('%d--%d', ci_lo, ci_high)) %>%
  knitr::kable(format='latex', escape = FALSE)

@ 
\caption{Percentage of boundary shift from typical talker to each exposure
  talker, averaged over subjects with 95\% bootstrapped confidence intervals
  (compare with Figure~\ref{fig:class-curves}).  0\% shift corresponds to no
  adaptation at all, while 100\% corresponds to perfect adaptation, ignoring any
  prior beliefs.}
  \label{tab:boundary-shift}
\end{table}

Figure~\ref{fig:class-curves} shows the classification functions for each
individual listener. In each accent, these classification functions tend to fall
in between the boundaries predicted by the typical talker distributions and the
boundaries implied by the exposure distributions.  We can quantify this by the
percentage of the predicted shift in category boundary from the classification
function for the typical talker to the boundary implied by the input
distribution (Table~\ref{tab:boundary-shift}). A 0\% shift corresponds to no
adaptation at all, while a shift of 100\% corresponds to complete adaptation to
the exposure distributions, with no (remaining) influence of the typical talker.

In all conditions, the average shift percentage was between 0\% and 100\%
(except the 0ms shift condition, which is so close to the typical talker that
estimating the percentage is numerically unstable). More interestingly, the more
extreme conditions show less complete adaptation than the less extreme conditions.
Together, these results suggest that listeners' adaptation was constrained by
their prior expectations (given the finite amount of evidence they received
about the unfamiliar talker).  This provides qualitative evidence that listeners
combine their prior expectations with observed cue distributions in order to
rapidly adapt to unfamiliar talkers, as predicted by the ideal adapter framework
\cite{Kleinschmidt2015}.

\section{Model}
\label{sec:model}

Our second goal in this paper is to test whether it is possible to infer
listeners prior beliefs based on their patterns of adaptation to different
accents. To that end, we use a variant of a Bayesian belief-updating model that
has previously provided a good account of how listeners incrementally update
their beliefs in order to rapidly adapt to an unfamiliar talker
\cite{Kleinschmidt2011, Kleinschmidt2012c, Kleinschmidt2015}. Previous modeling
work along these lines has treated the content of listeners prior beliefs---the
category means and variances they think are most likely---as known and fixed,
setting them based on pre-adaptation classification data, and then fitting the
confidence in those prior beliefs as a free parameter. Here, we wish to fit both
the content of (prior expected mean and variance of each category) and the
confidence in prior beliefs, based on adaptation data presented above. 

\subsection{Methods}
\label{sec:methods-model}

We denote the listener's beliefs about the \emph{current} talker's generative
model as the parameters of a two-component mixture of gaussians
\begin{equation}
  \label{eq:1}
  \theta = \{ \mu_\mathrm{b}, \sigma^2_\mathrm{b}, \mu_\mathrm{p},
              \sigma^2_\mathrm{p} \}
\end{equation}
where $\mu$ is a category's mean VOT and $\sigma^2$ is its variance.  As in
\citeA{Kleinschmidt2015}, we use an independent, conjugate Normal-$\chi^2$ prior
for each category, with parameters \cite{Gelman2003}
\begin{eqnarray}
  \label{eq:2}
  \phi &=& \{ \mu_{0,\mathrm{b}}, \sigma^2_{\mathrm{b}}, 
            \mu_{0,\mathrm{p}}, \sigma^2_{\mathrm{p}},
            \kappa_0, \nu_0 \} \\
  \label{eq:3}
  \theta | \phi &\sim& \prod_{c \in \{\mathrm{b,p}\}} 
    \mathrm{Normal}(\mu_c | \mu_{0,c}, \frac{\sigma_c^2}{\kappa_0})
    \chi^{-2} (\sigma_c^2 | \sigma^2_{0,c}, \nu_0)
\end{eqnarray}
where $\mu_{0}$ and $\sigma^2_0$ are a category's prior expected mean VOT and
variance, respectively, and $\kappa_0$ and $\nu_0$ are the listener's confidence
in these prior expectations, measured as pseudo-counts.  
Note that, as in previous modeling work in this framework, the prior confidence
parameters are shared between the two categories. Preliminary simulations showed
that it wasn't possible to uniquely identify the model using separate prior
confidence parameters for the two categories.

To estimate the listeners' prior beliefs, we infer values for these parameters
given the observed adaptation behavior (category responses $y$ and input VOTs
$x$) using Bayesian inference, marginalizing over $\theta$:

\begin{eqnarray}
  \label{eq:5}
  p(\phi | x, y) &\propto& p(y | \phi, x) p(\phi) \\
                 &\propto& \int \ud \theta p(y | \theta, x) p(\theta | x, \phi)
                           p(\phi)
\end{eqnarray}

We make the simplifying assumptions that the order of the trials does not matter
(exchangeability), and that for the purposes of belief updating
($p(\theta | x, \phi)$) the category labels that we, the experimenters, assigned
to each cluster are known.  This is equivalent to assuming that listeners pick
up on the cluster structure of the input they receive, accurately detecting the
mean and variance of each cluster, and finding a compromise between these
observed statistics and their prior beliefs, possibly revising earlier decisions
they made about assigning stimuli to one category or the other.  Backing off
this assumption is possible but computationally trickier, and we leave it as a
question for future work.  It does not, as far as we can determine, bias our
results one way or the other.  Finally, we also add a small lapsing rate
parameter, that allows for some proportion of responses to be attributed to
random guessing (see \citeNP{Clayards2008} for a discussion).

The posterior distributions of each of these parameters (the shared prior
beliefs plus lapsing rate) were estimated using MCMC with the Stan software
package \cite{Stan2015}.  Weakly informative hyperpriors were used that were
centered at 0 with standard deviations of 100 for the prior expected means and
variances (making them roughly constant over reasonable values) and 888 (four
times the total number of trials that listeners heard) for the prior confidence
pseudocounts (covering the whole range from completely ignoring the prior to
never adapting at all). The prior for the lapsing rate was uniform on $[0,1]$.
We ran four chains for 1000 samples each, discarding the first 500 as burn-in
for a total of 2000 samples overall. This sampler converged well and achieved
good mixing (maximum Gelman-Rubin $\hat{R}= \Sexpr{round(max_Rhat, 3)}$;
\citeNP{Gelman1992}).

\subsection{Results}
\label{sec:results-model}

<<run-model>>=

data_stan_conj <- data %>%
  supunsup::supunsup_to_stan_conj()

## mod_lapsing <- stan('../../modeling/stan/conj_id_lapsing_fit.stan')

## fit_lapsing <- stan(fit = mod_lapsing,
##                     data = data_stan_conj,
##                     chains = 4,
##                     iter = 1000)

## head(summary(fit_lapsing)$summary, n=10)

## summary_lapsing <- summary(fit_lapsing)$summary
## samples_lapsing <- rstan::extract(fit_lapsing)

## saveRDS(samples_lapsing, file='data/samples_lapsing.rds')
## saveRDS(summary_lapsing, file='data/summary_lapsing.rds')

mod_samples <- readRDS('data/samples_lapsing.rds')
mod_summary <- readRDS('data/summary_lapsing.rds')

## rename dimensions to make melting easier
rename_dims <- function(x, var, new_names) {
  names(dimnames(x[[var]])) <- new_names
  return(x)
}

mod_samples %<>%
  rename_dims('mu_0', c('iterations', 'cat_num')) %>%
  rename_dims('sigma_0', c('iterations', 'cat_num')) %>%
  rename_dims('mu_n', c('iterations', 'cat_num', 'subject_num')) %>%
  rename_dims('sigma_n', c('iterations', 'cat_num', 'subject_num')) %>%
  rename_dims('kappa_n', c('iterations', 'cat_num', 'subject_num')) %>%
  rename_dims('nu_n', c('iterations', 'cat_num', 'subject_num'))
  

max_Rhat <- max(mod_summary[, 'Rhat'])
lapse_rate <- mean(mod_samples$lapse_rate)

## correspondence between subject IDs and subject indices in model
subjects <-
  data %>%
  mutate(subject_num = as.numeric(factor(subject))) %>%
  group_by(subject, bvotCond, subject_num) %>%
  summarise() %>%
  arrange(subject_num)

categories <-
  data_frame(cat_num = 1:2,
             category = c('b', 'p'))

# helper function to melt a mult-dimensional array of samples into a df
melt_samples <- function(samlpes, varname) {
  reshape2::melt(mod_samples[[varname]], value.name=varname) %>%
    tbl_df
}

## create a data_frame with samples for prior parameters
prior_samples_df <- 
  c('mu_0', 'sigma_0', 'kappa_0', 'nu_0') %>%
  map( ~ melt_samples(mod_samples, .x)) %>%
  reduce(inner_join) %>%
  tbl_df %>%
  left_join(categories)

## create a data_frame with samples for updated parameters
updated_samples_df <- 
  c('mu_n', 'sigma_n', 'kappa_n', 'nu_n') %>%
  map( ~ melt_samples(mod_samples, .x)) %>%
  reduce(inner_join) %>%
  tbl_df %>%
  left_join(categories) %>%
  left_join(subjects) %>%
  group_by(bvotCond) %>%
  filter(subject == first(subject)) %>% # just need one per condition
  select(-cat_num, -subject_num, -subject)

## create a data_frame for lapsing rate samples
lapse_rate_samples <- melt_samples(mod_samples, 'lapse_rate')

@ 

<<model-goodness-of-fit>>=

mod_goodness_of_fit <- 
  data.frame(data_stan_conj$z_test_counts) %>%
  tbl_df() %>%
  mutate(prob_p = apply(mod_samples$p_test_conj[, , 2], 2, mean),
         prob_p_lapse = prob_p * (1-lapse_rate) + lapse_rate/2,
         LL_mod = dbinom(x = p, size = b+p, prob = prob_p_lapse, log = TRUE),
         LL_null = dbinom(x = p, size = b+p, prob = mean(p/(b+p)), log = TRUE)) %>%
  summarise(LL_mod = sum(LL_mod),
            LL_null = sum(LL_null),
            rho = cor(p/(b+p), prob_p_lapse, method='spearman'),
            n = n()) %>%
  mutate(LL_ratio = LL_mod - LL_null,
         pseudo_R2_mcfadden = 1 - LL_mod/LL_null,
         pseudo_R2_nagelkerke = (1 - exp(2/n * -LL_ratio)) / (1-exp(2/n*LL_null)))

@ 

\begin{figure*}[htb]
  \centering
<<model-classifications, fig.width=8, fig.height=2, out.width='\\textwidth'>>=

## pick a random subset of iterations to do the MCMC integration for posterior
## predictive checks
some_iterations <- 
  updated_samples_df %>%
  group_by(iterations) %>%
  summarise() %>%
  sample_n(200)

## convert samples into distributions and then classification functions for each
## condition
mod_class_funs <- 
  updated_samples_df %>%
  right_join(some_iterations) %>%
  mutate(mean=mu_n, sd=sigma_n) %>%
  select(iterations, bvotCond, category, mean, sd) %>%
  group_by(iterations, bvotCond) %>%
  do(stats_to_lhood(., noise_sd=0)) %>%
  lhood_to_classification() %>%
  left_join(lapse_rate_samples) %>%
  mutate(prob_p = (1-lapse_rate)*prob_p + lapse_rate/2) %>%
  select(bvotCond, vot, prob_p) %>%
  group_by(bvotCond, vot) %>%
  summarise(prob_p_low = quantile(prob_p, 0.025),
            prob_p_high = quantile(prob_p, 0.975),
            prob_p = mean(prob_p))

data_by_subject <- data %>%
  group_by(subject, bvotCond, vot) %>%
  summarise(prob_p = mean(respP))



## plot observed and model-predicted classification functions
ggplot(mod_class_funs, aes(x=vot, y=prob_p, color=bvotCond, fill=bvotCond)) +
  geom_ribbon(aes(ymin=prob_p_low, ymax=prob_p_high), size=0, alpha=0.5) + 
  geom_point(data = data_by_subject, stat='summary', fun.y='mean') + 
  geom_linerange(data=data_by_subject, stat='summary', fun.data='mean_cl_boot') + 
  facet_grid(.~bvotCond) +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Probability /p/ response') + 
  scale_color_discrete('/b/ mean\nVOT') + 
  scale_fill_discrete('/b/ mean\nVOT')

@ 
\caption{The classification functions (shaded ribbons, 95\% posterior predictive
  intervals) predicted by the belief updating model fit listeners' responses
  well (dots with lines showing bootstrapped 95\% confidence intervals).}
\label{fig:model-fit-classifications}
\end{figure*}

The first way we evaluate this model is to ask how well it fits listeners'
behavior. Figure~\ref{fig:model-fit-classifications} shows listeners' average
classification functions, compared with the posterior predictive classification
functions from the belief-updating model.  The first thing to notice is that the
model fits the data well (log-likelihood ratio vs. an intercept-only binomial
null model of \Sexpr{round(mod_goodness_of_fit[['LL_ratio']])} and Spearman's
$\rho$ = \Sexpr{mod_goodness_of_fit[['rho']]}) capturing the different
classification functions that result from exposure to each input distribution.
This in and of itself is an interesting result: it shows that there does exist
some set of prior beliefs such that the range of adaptation behavior we observed
can be explained by a model where all listeners start from a common set of prior
beliefs.

\begin{figure}[htb]
  \centering
<<inferred-prior, fig.width=4, fig.height=2, out.width='\\columnwidth'>>=

prior_summary <- 
  prior_samples_df %>% 
  gather('stat', 'val', mu_0:sigma_0) %>% 
  unite(stat_cat, stat, category) %>% 
  select(-cat_num) %>% spread(stat_cat, val) %>%
  gather('stat', 'value', kappa_0:sigma_0_p) %>% 
  group_by(stat) %>%
  summarise(mean=mean(value), 
            low=quantile(value, 0.025), 
            high=quantile(value, 0.975)) %>%
  mutate(units = ifelse(str_detect(stat, '(kappa|nu)'), 
                        'observations', 
                        'ms VOT'))

prior_expected <- prior_summary$mean
names(prior_expected) <- prior_summary$stat

prior_samples_df %>% 
  group_by(category) %>% 
  summarise(mean = mean(mu_0), sd = mean(sigma_0)) %>%
  stats_to_lhood(noise_sd = 0) %>%
  ggplot(aes(x=vot, y=lhood, group=category)) +
  geom_line(aes(linetype='Inferred\nprior')) +
  geom_line(data=prior_lhood, aes(linetype='Kronrod et\nal. (2012)')) +
  scale_linetype_discrete('Source') +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Likelihood')

@ 
  
\caption{Expected cue distributions based on inferred prior beliefs from
  adaptation data (solid lines). Plotted with VOT distributions measured by
  \protect\citeA{Kronrod2012} based on a combination of classification and
  discrimination behavior (dashed lines).}
  \label{fig:inferred-prior}
\end{figure}

\begin{table}[htb]
  \centering
<<inferred-prior-params, results='asis'>>=

format_params <- function(s) {
  s %>%
    str_replace('_0_([bp])$', '_{0,\\\\mathrm{\\1}}') %>%
    str_c('$\\', ., '$')
}

prior_summary %>% 
  transmute(Parameter = format_params(stat),
            Expected = mean,
            `95\\% HPD Int.` = sprintf('%.0f--%.0f', low, high),
            Units = units) %>%
  as.data.frame() %>%
  knitr::kable(format='latex', escape=FALSE)

@ 
  \caption{Expected values and 95\% highest posterior density intervals for the
    prior parameters, given the adaptation data.}
  \label{tab:prior-params}
\end{table}

The second way to evaluate this model is based on the prior beliefs it infers
listeners to have.  Table~\ref{tab:prior-params} shows the posterior expectation
and 95\% highest posterior density intervals for each of the prior belief
parameters given the adaptation data above.  The behavioral data was consistent
with high confidence in prior beliefs with the prior confidence about category
variances ($E(\nu_0) = \Sexpr{prior_expected['nu_0']}$) higher than confidence
in the category means ($E(\kappa_0) = \Sexpr{prior_expected['kappa_0']}$).  Both
of these (measured in pseudo-observations) are larger than the number of trials
that listeners heard in the experiment (222), which means that as far as the
belief-updating model is concerned, listeners updated beliefs reflected their
prior belies as much as (in the case of the means) or more than (for the
variances) the distributions they actually observed.  This is consistent with
the qualitative finding that listeners' category boundaries are intermediate
between the boundaires corresponding to a typical talker and the experimental
exposure talker.

Figure~\ref{fig:inferred-prior} shows the cue distributions corresponding to the
posterior expected values of the prior expected mean and variance parameters
given the behavioral data, compared with the distributions corresponding to a
typical talker (as determined by a combination of classification and
discrimination data, \citeNP{Kronrod2012}, see also \citeNP{Lisker1964}).  The
inferred prior beliefs are in reasonably good agreement with this typical
talker, with one excpetion: the mean for /b/ is slightly lower
($E(\mu_{0,\mathrm{b}}) = \Sexpr{prior_expected['mu_0_b']}$ ms), and the standard
deviation of /b/ is slightly higher ($E(\sigma_{0,\mathrm{b}}) =
\Sexpr{prior_expected['sigma_0_b']}$ ms).

\subsection{Discussion}
\label{sec:discussion-1}

These modeling results show that the pattern of adaptation behavior observed
above is consistent with a belief-updating model of phonetic adaptation that
combines prior expectations with input statistics in order to infer the current
talker's cue distributions.  Specifically, it shows that there exists a single
set of prior beliefs that captures the range of adaptation to different input
distributions that stretches from nearly complete adaptation to partial
adaptation at best.

These prior beliefs are reasonably consistent with other attempts to determine
what listeners think the underlying cue distributions are \cite{Kronrod2012}, as
well as the distributions produced by actual talkers \cite{Lisker1964}.  In
fact, the prior expected VOT distribution for /p/ that our model inferred is
almost identical to that observed by both \citeA{Kronrod2012} and
\citeA{Lisker1964}.  The distribution for /b/ deviates from prior work,
however. One possible reason for this is that a substantial minority of English
speakers produce prevoiced /b/ \cite{Lisker1964,Docherty2011}, which is
characterized by a lower (negative) VOT and a higher variance (often higher even
than /p/).  That is, across talkers, the /b/ VOT distribution parameters (mean
and variance) have a \emph{bimodal} distribution.  We assumed a single, unimodal
prior distrbution, and the prior beliefs we inferred to be most likely are
consistent with a compromise between the two types of /b/ distributions that
talkers actually produce.  

This possibility suggests two directions for future work. First, large-scale
corpus studies of VOT distributions would be informative about the true
underlying cross-talker variability in these distributions, but the only study
of this type we are aware of considers only non-negative VOTs
\cite{Chodroff2015} and thus excludes talkers who prevoice their /b/.  Second,
more modeling work is needed to test whether a multimodal prior is justified
given the adaptation data, and if so, whether it would change the inference
about listeners' prior expectations for /b/.

The other major result of this modeling is that listeners have high confidence
in their prior expectations about the VOT distributions of /b/ and /p/, acting
as if they had already observed around 200--800 samples from each category (for
the category means and variances, respectively) from the unfamiliar talker they
encountered in our experiemnt.  These confidence values are one or two orders of
magnitude higher than those inferred in other belief-updating modeling
\cite{Kleinschmidt2015}.

However, this previous work was based on adaptation to a /b/-/d/ contrast, which
is cued by spectral cues (formant frequency transitions) which generally vary
substantially across talkers \cite{Hillenbrand1995, Peterson1952} .  The
acoustic cues to the /b/-/p/ contrast used in the current study do not show as
much variability across talkers \cite{Allen2003, Chodroff2015}.  When there is
little variability across talkers, past experience with other talkers' VOT
distributions is highly informative about the distributions that an unfamiliar
talker will produce, requiring less adaptation.  Likewise, when there is more
variability across talkers, listeners need to rely more on the current talker's
cue distributions and less on their prior experience.  Thus, the apparent
discrepancy between the confidence that listeners place in their prior beliefs
in the current study and in \citeA{Kleinschmidt2015} is entirely consistent with
an ``ideal adapter'' which combines prior beliefs with current experience
weighted according to confidence \cite{Kleinschmidt2015}.  This idea finds
further empirical support in \citeA{Kraljic2007}, who found that after the same
amount of exposure, listeners recalibrate a /d/-/t/ contrast (analogous to the
/b/-/p/ contrast used here) much less than an /s/-\ph S contrast (where there is
substantial variability across talkers; \citeNP{McMurray2011a, Newman2001}).

\section{Conclusion}
\label{sec:conclusion}

A central prediction of the ideal adapter framework \cite{Kleinschmidt2015} is
that listeners adapt to unfamiliar talkers by combining their prior beliefs with
observed evidence about that particular talker's cue distributions.  In this
paper, we have shown first that for a range of different accents (cue
distributions), listeners' behavior in a distributional learning experiment
reflects a compromise between what would be expected for the cue distributions
produced by a typical talker and the exposure talker.  Second we have shown that
the range of adaptation behavior observed across the various accents listeners
heard can be captured by a single belief-updating model, which assumes that
listeners start from shared prior expectations and update them based on
experience with the exposure talker. 

These results emphasize the importance of listeners' prior expectations for
robust speech perception in the face of talker variability.  Even if all the
listener knows about the talker is that they are speaking English, they can
still benefit from prior experience with other speakers of English to provide an
informative head start for adaptation.
The modeling framework we use has the additional advantage of allowing us to
\emph{infer} what cue distributions listeners believe an unfamiliar talker will
produce. This provides a potentially powerful---and heretofore missing---tool
for probing listeners' prior expectations, based only on comprehension
data. These beliefs reflect what listeners have learned about the variability
they can expect across talkers, and probing how this internal model is
related to the \emph{actual} variability across talkers (measured via speech
production data) is an important next step in advancing our understanding of
robust speech perception.  

More generally, prior knowledge is increasingly understood to play in important
role in a number of perceptual and memory domains
\cite<e.g.,>{Brady2009,Brady2013,Froyen2015a,Orhan2011}.  Distributional
learning provides an approach to probing prior expectations about the
\emph{statistics} of the sensory world, which, as in speech perception, are
critical to effectively coping with non-stationarity in sensory statistics.

\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{/Users/dkleinschmidt/Documents/papers/library-clean}



\end{document}
