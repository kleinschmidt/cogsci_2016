\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage{siunitx}
\usepackage{dcolumn}

\usepackage{subcaption}

\usepackage{tipx}
\newcommand{\ph}[1]{\protect\textipa{/#1/}}
\newcommand{\aph}[1]{\protect\textipa{[#1]}}

\newcommand{\ms}[1]{\SI{#1}{\milli\second}}

\newcommand{\ud}{\mathrm{d}}

\title{What do you expect from an unfamiliar talker?}
 
\author{{\large \bf Dave F. Kleinschmidt$^1$}, and 
   {\large \bf T. Florian Jaeger$^{1,2,3}$} \\
   \texttt{\{dkleinschmidt, fjeager\} $@$ mail.bcs.rochester.edu} \\
  $^1$Department of Brain and Cognitive Sciences, $^2$Department of Computer Science, and $^3$Department of Linguistics, \\
  University of Rochester, Rochester, NY, 14627 USA}


\begin{document}

\maketitle

\begin{abstract}
  
  Speech perception is made much harder by variability between talkers. As a
  result, listeners need to adapt to each different talker's particular acoustic
  cue distributions. Thinking of this adaptation as a form of statistical
  inference, we explore the role that listeners' prior expectations play in
  adapting to an unfamiliar talker. Specifically, we test the hypothesis that
  listeners will have a harder time adapting to talkers whose cue distributions
  fall outside the range of normal variation across talkers. We also show that
  it is possible to \emph{infer} listeners' shared prior expectations based on
  patterns of adaptation to different cue distributions. This provides a
  potentially powerful tool for directly probing listeners' prior expectations
  about talkers that does not rely on speech produced by many different talkers,
  which is costly to collect and annotate, and only indirectly related to
  listeners' subjective expectations.
  
  \textbf{Keywords:} Cognitive Science, Linguistics, Psychology, Language
  understanding, Learning, Speech recognition, Bayesian modeling, Experimental
  research with adult humans
\end{abstract}



<<preamble, echo=FALSE, results='hide', warning=FALSE, message=FALSE>>=

knitr::opts_chunk$set(echo=FALSE,
                      results='hide',
                      cache=TRUE,
                      autodep=TRUE,
                      message=FALSE,
                      warning=FALSE,
                      error=FALSE)

options(scipen = 2, digits = 2)

library(supunsup)

library(dplyr)
library(tidyr)
library(magrittr)
library(stringr)

library(rstan)

data <- supunsup::supunsup_clean %>%
  filter(supCond == 'unsupervised') %>%
  mutate(subjNum = as.numeric(factor(subject)),
         trueCatNum = as.numeric(trueCat),
         respCatNum = as.numeric(respCat))

library(ggplot2)
theme_set(theme_bw())

library(lme4)

@

\section{Introduction}
\label{sec:introduction}

A longstanding problem in speech perception is how listeners manage to cope with
substantial differences in how individual talkers produce speech.  Recent
evidence suggests that one strategy listeners employ is to \emph{rapidly adapt}
to unfamiliar talkers \cite[among
others]{Bertelson2003,Clarke2004,Kraljic2007}.  Such adaptation can
be understood as a form of statistical inference. This insight is captured by a
recent proposal, the {\em ideal adapter} framework \cite{Kleinschmidt2015}.
Each talker's particular accent (way of talking) can be formalized as the
distribution of acoustic cues that they produce for each phonetic category (or
other underlying linguistic unit). Listeners are taken to adapt to an unfamiliar
talker via \emph{distributional learning}, inferring the underlying
talker-specific cue distributions from the talker's productions.

Critically, this statistical inference process draws on implicit beliefs about
{\em how} talkers tend to differ from each other. As a consequence, adaptation
to an unfamiliar talker should depend on a listener's prior experience with
other talkers, rather than only on the speech produced by the unfamiliar talker
themselves. Specifically, a listener's experience with other talkers provides
the starting point for the distributional learning required for adaptation, or,
in Bayesian terms, a \emph{prior belief} about the probability of different
possible accents (cue distributions).  More informative prior beliefs can
substantially reduce the amount of direct evidence needed to converge on
accurate beliefs about the current talker's cue distributions.

The goals of the present work are two-fold. First, we test a critical prediction
of the ideal adapter framework.  To the extent that a listener's prior beliefs
are informative, they must take some probability \emph{away} from unlikely
accents.  Confronted by a talker whose accent falls well outside the range of
what they expect based on their previous experience, the ideal adapter framework
predicts that a listener will require more evidence to adapt, leading to slowed
or incomplete adaptation.  There is some evidence that this is the case.  For
instance, \citeA{Idemaru2011} found that listeners have difficulty adapting to a
talker who produces anti-correlated distributions of two cues that are typically
positively or un-correlated.  \citeA{Sumner2011} found that listeners had
trouble adapting to a talker who produced a distribution of cues for the /b/ and
/p/ sounds that had substantially lower means than a typical talker.

However, no studies have systematically probed whether and how a listener's
prior expectations constrain phonetic adaptation, or even what kind of prior
beliefs listeners have.  To that end, we expose listeners to a range of
different accents, which differ (only) in the cue distributions for /b/ and /p/.
By parametrically manipulating these distributions, we create a range of accents
that are more or less similar to what a typical talker of English produces. We
then assess the degree to which listeners adapt their beliefs about the novel
talker's cue distributions, depending on the {\em a priori} typicality of these
distributions.

To anticipate the results, we find that typicality of the novel talker's cue
distribution predicts the degree to which listeners adapt to the talker. This
suggests that listeners not only have beliefs about the cue distributions for a
\emph{particular} single talker (as suggested by previous work;
\citeNP{Clayards2008, Feldman2009, Kleinschmidt2015, Kronrod2012}), but also
have implicit beliefs about the ways in which talkers tend to \emph{differ} from
each other, and hence what to expect from an unfamiliar talker. This leads to
the second question we address here: what is the content of listeners' prior
beliefs about inter-talker variability? To this end, we use a Bayesian
belief-updating model to work backwards from listeners' adaptation behavior
across talkers and \emph{infer} listeners' shared prior beliefs.  By inferring
these priors from {\em perception} data, this approach has the potential to
avoid the need for costly and time-consuming annotation of production data from
many different talkers. This would provide a powerful method for investigating
listeners' prior beliefs that could be applied to other cues, categories, and
even social variables (like gender, native language background, etc.).

\section{Experiment}
\label{sec:experiment}

\begin{figure*}[!htb]
\centering

<<input-vs-prior-stats, fig.width=8, fig.height=2, out.width="\\textwidth">>=

## copied from NIPS paper #######################################

## prior parameters from Kronrod et al. (CogSci 2012)
prior_stats <- data.frame(category=factor(c('b', 'p')),
                          mean = c(0, 60),
                          sd = sqrt(c(14, 254)))

## prior parameters from Chodroff et al. ICPhS 2015
## prior_stats <- data.frame(category=factor(c('b', 'p')),
##                           mean = c(9, 51),
##                           sd = c(5, 21))


exposure_stats <- data %>%
  group_by(bvotCond, category=trueCat) %>%
  summarise(mean=mean(vot), sd=sd(vot))

sd_noise = sqrt(82)

stats_to_lhood <- function(stats, noise_sd=sd_noise) {
  stats %>%
    group_by(category, mean, sd) %>%
    do(data.frame(vot=seq(-30, 90, 0.5))) %>%
    ungroup() %>%
    mutate(lhood = dnorm(vot, mean, sqrt(sd^2 + noise_sd^2))) %>%
    select(-mean, -sd)
}

exposure_lhood <- exposure_stats %>%
  group_by(bvotCond) %>%
  do(stats_to_lhood(., sd_noise))

prior_lhood <- prior_stats %>% stats_to_lhood(sd_noise)

data %>%
  group_by(bvotCond, vot) %>%
  filter(subject == first(subject)) %>%
  tally() %>%
  ggplot(aes(x=vot)) +
  geom_bar(stat='identity', aes(y=n, fill=bvotCond)) +
  geom_line(data=prior_lhood, aes(y=lhood*1600, group=category),
            color="black", linetype=2) +
  geom_text(data=data.frame(bvotCond=-10), x = 10, y = 60,
            label = 'Typical Talker',
            color='black', hjust=0, vjust=0.3, size=3) +
  geom_text(data=data.frame(bvotCond=-10), x = 40, y = 50,
            label = 'Exposure\nTalker',
            color=hcl(h=15, c=100, l=65), hjust=0, vjust=0.8, size=3,
            lineheight=1) + 
  facet_grid(.~bvotCond) +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Frequency') +
  scale_fill_discrete('/b/ mean\nVOT') ## +
  ## theme(legend.position='none')

@ 

\caption{Each subject heard one of these five synthetic accents, which differ
  only in the distribution of VOTs of the word-initial stops. Black dashed lines
  show VOT distributions from a hypothetical typical talker (as estimated by
  \citeNP{Kronrod2012}). Note that the 0 and 10ms shifted accents are reasonably
  close to this typical talker, while the $-10$, 20, and 30ms shifted accents
  deviate substantially.}
\label{fig:vot-dists}

\end{figure*}

We tested the role that listeners prior expectations play in adapting to an
unfamiliar talker by exposing them to one of five synthetic ``accents''
(Figure~\ref{fig:vot-dists}). These accents differed only in the distribution of
voice onset time (VOT), the primary cue to word-initial stop consonant voicing
in English (e.g., ``beach'' vs. ``peach''). Adaptation was assessed based on
listeners' classification function, or how they labeled each VOT as /b/ or /p/.

\subsection{Methods}
\label{sec:methods}

\subsubsection{Subjects}
\label{sec:subjects}

<<participants>>=
n_subj <- data %>% group_by(subject) %>% summarise() %>% tally()

excluded <- supunsup::supunsup_excluded %>%
  filter(supCond == 'unsupervised') %>%
  group_by(subject) %>% 
  summarise() %>% 
  right_join(supunsup::excludes) %>%
  select(subject, exclude80PercentAcc, rank)

n_excluded <- nrow(excluded)
n_subj_repeat <- sum(!is.na(excluded$rank))
n_subj_bad <- sum(!is.na(excluded$exclude80PercentAcc))
n_both <- n_subj_repeat + n_subj_bad - n_excluded 

n_total <- n_subj + n_excluded

@ 

We recruited \Sexpr{n_total} subjects via Amazon's Mechanical Turk, who were
paid \$2.00 for participation, which took about 20 minutes. We excluded subjects
who participated more than once ($n=\Sexpr{n_subj_repeat}$) or whose accuracy at
\ms{0} and \ms{70} VOT---as extrapolated via a logistic GLM---was less than 80\%
correct ($n=\Sexpr{n_subj_bad}$; $n=\Sexpr{n_both}$ for both reasons). Excluded
subjects were roughly equally distributed across conditions (maximum of 5 in 0ms
/b/ VOT condition, and minimum of 1 in 20ms /b/ VOT condition). After these
exclusions, data from \Sexpr{n_subj} subjects remained for analysis for
analysis.

\subsubsection{Procedure}
\label{sec:procedure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\columnwidth]{figure_manual/beach_peach.png}
  \caption{Example trial display (beach/peach). Listeners first click on the
    green button to play the word, then click on one picture to indicate what
    they heard.}
  \label{fig:beach-peach}
\end{figure}


Our distributional learning procedure is described in \citeA{Kleinschmidt2015a},
and is based on \citeA{Clayards2008}. On each trial, two response option images
appeared, which corresponded to one of three /b/-/p/ minimal pairs (beach/peach,
bees/peas, or beak/peak). Subjects then clicked on a central button which played
the corresponding minimal pair word, and then clicked on the picture to indicate
whether they heard the /b/ or /p/ member of the minimal pair
(Figure~\ref{fig:beach-peach}).  Subjects performed 222 of these trials.

Each trial's word was synthesized with a voice onset time (VOT) that was
randomly drawn from a bimodal distribution, with low and high VOT clusters
implicitly corresponding to /b/ and /p/, respectively.  This distribution
defined the \emph{accent} that the subject heard, and each subject was
pseudorandomly assigned to one of five accent conditions
(Figure~\ref{fig:vot-dists}).  Each of these accents implies a different /b/-/p/
category boundary, and listeners' adaptation was evaluated by comparing their
/b/-/p/ classification function (fitted by a logistic GLM) to the classification
function derived from the typical talker's cue distributions (corresponding to
no adaptation) and the exposure distribution (corresponding to maximal---but not
necessarily optimal---adaptation).

\subsection{Results and Discussion}
\label{sec:results}

<<regression, eval=FALSE>>=
data_lmer <- supunsup::mutate_for_lmer(data)

## double check that contrasts are helmert coded...
contrasts(data_lmer$bvotCond)

## basically anything more than this doesn't converge...
mod <- glmer(respP ~ vot_rel.s * trial.s + bvotCond + (1 | subject),
             data_lmer, family='binomial')

@ 

\begin{figure*}[htb]
  \centering

<<class-curves, fig.height=2, fig.width=8, out.width="\\textwidth">>=
## generate predicted classification functions assuming Bayes-optimal classifier
## + noise

lhood_to_classification <- function(lhood) {
  lhood %>%
    spread(category, lhood) %>%
    mutate(prob_p = p / (p+b))
}

perfect_learning <- exposure_stats %>%
  group_by(bvotCond) %>%
  do(stats_to_lhood(.)) %>%
  lhood_to_classification

no_learning <- prior_stats %>%
  stats_to_lhood %>%
  lhood_to_classification

prior_bound <- no_learning %>%
  arrange(abs(prob_p - 0.5)) %>%
  filter(row_number() ==1) %$%
  vot


boundaries <- data %>%
  group_by(bvotCond, subject) %>%
  do({ glm(respP ~ vot, family='binomial', data=.) %>%
         broom::tidy() %>%
         select(term, estimate)
  }) %>%
  ungroup() %>%
  spread(term, estimate) %>%
  mutate(boundary = -`(Intercept)` / vot,
         ideal_boundary = as.numeric(as.character(bvotCond)) + 20,
         prior_boundary = prior_bound,
         prop_shift = (boundary-prior_boundary)/(ideal_boundary-prior_boundary))

boundary_summary <- boundaries %>%
  group_by(bvotCond) %>%
  summarise(median_shift_perc = round(100*median(prop_shift)),
            shift_text = paste(median_shift_perc, '%', sep='')) %>%
  filter(bvotCond != 0)                 # basically no shift possible

ggplot(data, aes(x=vot, y=respP, color=bvotCond)) +
  geom_line(aes(group=subject), stat='smooth', method='glm', 
            method.args=list(family='binomial'), alpha=0.2) +
  facet_grid(.~bvotCond) +
  geom_line(data=perfect_learning, aes(y=prob_p), group=1, linetype=2, size=1) +
  geom_line(data=no_learning, aes(y=prob_p), group=1, linetype=2, color='black') +
  geom_text(data=data.frame(bvotCond=-10),
            x = 30, y = 0, label = 'Typical\ntalker',
            size = 3.5, hjust=0, vjust = 0, color='black',
            lineheight=1) + 
  geom_text(data=data.frame(bvotCond=-10),
            x = 12, y = 1, label = 'Expo-\nsure',
            size = 3.5, hjust=1, vjust=1, color=hcl(15, c=100, l=65),
            lineheight=1, fontface='bold') + 
  geom_text(data=data.frame(bvotCond=-10),
            x = 90, y = 0.75, label = 'Actual\nlisteners',
            size = 3.5, hjust=1, vjust=1, color=hcl(15, c=100, l=65),
            lineheight=1) + 
  ## geom_text(data=boundary_summary, aes(x=75, y=0.1, label=shift_text), color='black') + 
  ## theme(legend.position='none') +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Probability /p/ response') + 
  scale_color_discrete('/b/ mean\nVOT')


@ 
  
\caption{Listeners' responses, smoothed with logistic functions (thin lines),
  compared with the classification functions expected based on a typical talker
  (no learning; dashed black lines) and complete adaptation to the exposure
  distributions (thick dashed colored lines). Listeners' actual category
  boundaries lie between the typical talker and exposure talker boundaries (see
  Table~\ref{tab:boundary-shift}).}
\label{fig:class-curves}
\end{figure*}


<<shift-by-third, eval=FALSE>>=

## Do boundary shift analysis by thirds
boundaries_by_third <-
  data %>%
  mutate(thirds = ntile(trial, 3)) %>%
  group_by(bvotCond, subject, thirds) %>%
  do({ glm(respP ~ vot, family='binomial', data=.) %>%
         broom::tidy() %>%
         select(term, estimate)
  }) %>%
  ungroup() %>%
  spread(term, estimate) %>%
  mutate(boundary = -`(Intercept)` / vot,
         ideal_boundary = as.numeric(as.character(bvotCond)) + 20,
         prior_boundary = prior_bound,
         prop_shift = (boundary-prior_boundary)/(ideal_boundary-prior_boundary)) %>%
  group_by(bvotCond, thirds) %>% 
  do(daver::boot_ci(., function(d,i) {mean(d$prop_shift[i])}))
 
ggplot(boundaries_by_third, 
       aes(x=thirds, y=observed, ymin=ci_lo, ymax=ci_high, 
           color=bvotCond, fill=bvotCond)) + 
  geom_ribbon(alpha=0.2, size=0) + 
  geom_line() + 
  facet_grid(.~bvotCond)

@ 

Figure~\ref{fig:class-curves} shows the classification functions for each
individual listener. In each accent, these classification functions tend to fall
in between the boundaries predicted by the typical talker distributions and the
boundaries implied by the exposure distributions.  We can quantify this by the
percentage of the predicted shift in category boundary from the classification
function for the typical talker to the boundary implied by the input
distribution (Table~\ref{tab:boundary-shift}). A 0\% shift corresponds to no
adaptation at all, while a shift of 100\% corresponds to complete adaptation to
the exposure distributions, with no (remaining) influence of any prior beliefs.

In all conditions, the average shift percentage was between 0\% and 100\%
(except the 0ms shift condition, which is so close to the typical talker that
estimating the percentage is numerically unstable). More interestingly, the more
extreme conditions show less complete adaptation than the less extreme conditions.
Together, these results suggest that listeners' adaptation was constrained by
their prior expectations (given the finite amount of evidence they received
about the unfamiliar talker).  This provides qualitative evidence that listeners
combine their prior expectations with observed cue distributions in order to
rapidly adapt to unfamiliar talkers, as predicted by the ideal adapter framework
\cite{Kleinschmidt2015}.

\begin{table}[tb]
  \centering
<<boundary-shift-table, results='asis'>>=

## bootstrapped boundary summary
boundaries %>% 
  group_by(bvotCond) %>% 
  do(daver::boot_ci(., function(d,i) {mean(d$prop_shift[i])})) %>%
  mutate(observed = round(observed * 100),
         ci_lo = round(ci_lo * 100),
         ci_high = round(ci_high * 100)) %>%
  ungroup() %>%
  transmute(`/b/ mean VOT` = bvotCond,
            `Mean shift` = ifelse(bvotCond == 0,
                                  '---',
                                  sprintf('%d\\%%', observed)),
            `95\\% CI` = ifelse(bvotCond == 0,
                                '---',
                                sprintf('%d--%d\\%%', ci_lo, ci_high))) %>%
  knitr::kable(format='latex', escape = FALSE)

@ 
\caption{Percentage of boundary shift from typical talker to each exposure
  talker (see Figure~\ref{fig:class-curves}), averaged over subjects with 95\%
  bootstrapped confidence intervals.  0\% shift corresponds to no adaptation at
  all, while 100\% corresponds to perfect adaptation, ignoring any prior
  beliefs. Typical and exposure talker boundaries were too close together to
  reliably determine boundary shift percentage in the 0ms condition.}
  \label{tab:boundary-shift}
\end{table}


\section{Inferring prior beliefs about talker variability} %% [TFJ: how about ``Inferring priors about inter-talker variability'' or something like that?]
\label{sec:model}

Our second goal in this paper is to test whether it is possible to infer
listeners' prior beliefs about talker variability,
%% [TFJ: here and elsewhere: I think it's important to be clear what type of prior beliefs you mean; hence 
%% also my edits in the intro; there and in the discussion, I think it's particularly important to be clear that you're not just
%% inferring talker-marginalized category-specific cue distributions] 
based on their patterns of adaptation to different
accents. To that end, we use a variant of a Bayesian belief-updating model that
has previously provided a good account of how listeners incrementally update
their beliefs in order to rapidly adapt to an unfamiliar talker
\cite{Kleinschmidt2015}. This previous modeling
work has treated the content of listeners prior beliefs---the
category means and variances they think are most likely---as known and fixed,
setting them based on pre-adaptation classification data, and then fitting the
confidence in those prior beliefs as a free parameter. Here, we wish to fit both
the content of (prior expected mean and variance of each category) and the
confidence in prior beliefs, based on adaptation data presented above. 

\subsection{Methods}
\label{sec:methods-model}

We denote the listener's beliefs about the \emph{current} talker's generative
model as the parameters of a two-component mixture of gaussians
\begin{equation}
  \label{eq:1}
  \theta = \{ \mu_\mathrm{b}, \sigma^2_\mathrm{b}, \mu_\mathrm{p},
              \sigma^2_\mathrm{p} \}
\end{equation}
where $\mu$ is a category's mean VOT and $\sigma^2$ is its variance.  As in
\citeA{Kleinschmidt2015}, we use an independent, conjugate Normal-$\chi^{-2}$ prior
for each category, with parameters \cite{Gelman2003}
\begin{eqnarray}
  \label{eq:2}
  \phi &=& \{ \mu_{0,\mathrm{b}}, \sigma^2_{\mathrm{b}}, 
            \mu_{0,\mathrm{p}}, \sigma^2_{\mathrm{p}},
            \kappa_0, \nu_0 \} \\
  \label{eq:3}
  \theta | \phi &\sim& \prod_{c \in \{\mathrm{b,p}\}} 
    \mathrm{Normal}(\mu_c | \mu_{0,c}, \frac{\sigma_c^2}{\kappa_0})
    \chi^{-2} (\sigma_c^2 | \sigma^2_{0,c}, \nu_0)
\end{eqnarray}
where $\mu_{0}$ and $\sigma^2_0$ are a category's prior expected mean VOT and
variance, respectively, and $\kappa_0$ and $\nu_0$ are the listener's confidence
in these prior expectations, measured as pseudo-counts.  
Note that, as in previous modeling work in this framework, these prior confidence
parameters are shared between the two categories. Preliminary simulations showed
that it wasn't possible to uniquely identify the model using separate prior
confidence parameters for the two categories.

To estimate the listeners' prior beliefs, we infer values for these parameters
given the observed adaptation behavior (category responses $y$ and input VOTs
$x$) using Bayesian inference, marginalizing over $\theta$:

\begin{eqnarray}
  \label{eq:5}
  p(\phi | x, y) &\propto& p(y | \phi, x) p(\phi) \\
                 &\propto& \int \ud \theta p(y | \theta, x) p(\theta | x, \phi)
                           p(\phi)
\end{eqnarray}

We make the simplifying assumptions that the order of the trials does not
matter for the current purpose, and that %for the purposes of belief updating ($p(\theta | x, \phi)$) 
the category labels that we, the experimenters, assigned to each cluster are known.
This is equivalent to assuming that listeners pick up on the cluster structure
of the input they receive, accurately detecting the mean and variance of each
cluster, and find a compromise between these observed statistics and their
prior beliefs, possibly revising earlier decisions they made about assigning
stimuli to one category or the other.  Backing off this assumption is possible
but computationally more demanding, and we leave it as a question for future work. 
This simplifying assumption does not, as far as we can determine, bias our results.
Finally, we also add a small lapse rate parameter, that allows for some
proportion of responses to be attributed to random guessing (e.g., because of 
attentional blinks, see \citeNP{Clayards2008} for a discussion).

<<run-model>>=

data_stan_conj <- data %>%
  supunsup::supunsup_to_stan_conj()

## mod_lapsing <- stan('../../modeling/stan/conj_id_lapsing_fit.stan')

## fit_lapsing <- stan(fit = mod_lapsing,
##                     data = data_stan_conj,
##                     chains = 4,
##                     iter = 1000)

## head(summary(fit_lapsing)$summary, n=10)

## summary_lapsing <- summary(fit_lapsing)$summary
## samples_lapsing <- rstan::extract(fit_lapsing)

## saveRDS(samples_lapsing, file='data/samples_lapsing.rds')
## saveRDS(summary_lapsing, file='data/summary_lapsing.rds')

mod_samples <- readRDS('data/samples_lapsing.rds')
mod_summary <- readRDS('data/summary_lapsing.rds')

## rename dimensions to make melting easier
rename_dims <- function(x, var, new_names) {
  names(dimnames(x[[var]])) <- new_names
  return(x)
}

mod_samples %<>%
  rename_dims('mu_0', c('iterations', 'cat_num')) %>%
  rename_dims('sigma_0', c('iterations', 'cat_num')) %>%
  rename_dims('mu_n', c('iterations', 'cat_num', 'subject_num')) %>%
  rename_dims('sigma_n', c('iterations', 'cat_num', 'subject_num')) %>%
  rename_dims('kappa_n', c('iterations', 'cat_num', 'subject_num')) %>%
  rename_dims('nu_n', c('iterations', 'cat_num', 'subject_num'))
  

max_Rhat <- max(mod_summary[, 'Rhat'])
lapse_rate <- mean(mod_samples$lapse_rate)

## correspondence between subject IDs and subject indices in model
subjects <-
  data %>%
  mutate(subject_num = as.numeric(factor(subject))) %>%
  group_by(subject, bvotCond, subject_num) %>%
  summarise() %>%
  arrange(subject_num)

categories <-
  data_frame(cat_num = 1:2,
             category = c('b', 'p'))

# helper function to melt a mult-dimensional array of samples into a df
melt_samples <- function(samlpes, varname) {
  reshape2::melt(mod_samples[[varname]], value.name=varname) %>%
    tbl_df
}

## create a data_frame with samples for prior parameters
prior_samples_df <- 
  c('mu_0', 'sigma_0', 'kappa_0', 'nu_0') %>%
  map( ~ melt_samples(mod_samples, .x)) %>%
  reduce(inner_join) %>%
  tbl_df %>%
  left_join(categories)

## create a data_frame with samples for updated parameters
updated_samples_df <- 
  c('mu_n', 'sigma_n', 'kappa_n', 'nu_n') %>%
  map( ~ melt_samples(mod_samples, .x)) %>%
  reduce(inner_join) %>%
  tbl_df %>%
  left_join(categories) %>%
  left_join(subjects) %>%
  group_by(bvotCond) %>%
  filter(subject == first(subject)) %>% # just need one per condition
  select(-cat_num, -subject_num, -subject)

## create a data_frame for lapsing rate samples
lapse_rate_samples <- melt_samples(mod_samples, 'lapse_rate')

@ 

The posterior distributions of each of these parameters (the shared prior
beliefs plus lapsing rate) were estimated using MCMC with the Stan software
package \cite{Stan2015}.  Weakly informative hyperpriors were used that were
centered at 0 with standard deviations of 100 for the prior expected means and
variances (making them roughly constant over reasonable values) and 888 (four
times the total number of trials that listeners heard) for the prior confidence
pseudocounts (which is essentially uniform on the whole range from completely
ignoring prior beliefs to not adapting at all).  The prior for the lapsing rate
was uniform on $[0,1]$.  We ran four chains for 1000 samples each, discarding
the first 500 as burn-in for a total of 2000 samples overall. This sampler
converged well and achieved good mixing (maximum
$\hat{R}= \Sexpr{round(max_Rhat, 3)}$; \citeNP{Gelman1992}).

\subsection{Results}
\label{sec:results-model}


<<model-goodness-of-fit>>=

mod_fitted <- 
  data.frame(data_stan_conj$z_test_counts) %>%
  tbl_df() %>%
  mutate(prob_p = apply(mod_samples$p_test_conj[, , 2], 2, mean),
         prob_p_lapse = prob_p * (1-lapse_rate) + lapse_rate/2)

mod_goodness_of_fit <- 
  mod_fitted %>%
  mutate(LL_mod = dbinom(x = p, size = b+p, prob = prob_p_lapse, log = TRUE),
         LL_null = dbinom(x = p, size = b+p, prob = mean(p/(b+p)), log = TRUE)) %>%
  summarise(LL_mod = sum(LL_mod),
            LL_null = sum(LL_null),
            rho = cor(p/(b+p), prob_p_lapse, method='spearman'),
            n = n()) %>%
  mutate(LL_ratio = LL_mod - LL_null,
         pseudo_R2_mcfadden = 1 - LL_mod/LL_null,
         pseudo_R2_nagelkerke = (1 - exp(2/n * -LL_ratio)) / (1-exp(2/n*LL_null)))

@ 

\begin{figure*}[htb]
  \centering
<<model-classifications, fig.width=8, fig.height=2, out.width='\\textwidth'>>=

## pick a random subset of iterations to do the MCMC integration for posterior
## predictive checks
some_iterations <- 
  updated_samples_df %>%
  group_by(iterations) %>%
  summarise() %>%
  sample_n(200)

## convert samples into distributions and then classification functions for each
## condition
mod_class_funs <- 
  updated_samples_df %>%
  right_join(some_iterations) %>%
  mutate(mean=mu_n, sd=sigma_n) %>%
  select(iterations, bvotCond, category, mean, sd) %>%
  group_by(iterations, bvotCond) %>%
  do(stats_to_lhood(., noise_sd=0)) %>%
  lhood_to_classification() %>%
  left_join(lapse_rate_samples) %>%
  mutate(prob_p = (1-lapse_rate)*prob_p + lapse_rate/2) %>%
  select(bvotCond, vot, prob_p) %>%
  group_by(bvotCond, vot) %>%
  summarise(prob_p_low = quantile(prob_p, 0.025),
            prob_p_high = quantile(prob_p, 0.975),
            prob_p = mean(prob_p))

data_by_subject <- data %>%
  group_by(subject, bvotCond, vot) %>%
  summarise(prob_p = mean(respP))



## plot observed and model-predicted classification functions
ggplot(mod_class_funs, aes(x=vot, y=prob_p, color=bvotCond, fill=bvotCond)) +
  geom_ribbon(aes(ymin=prob_p_low, ymax=prob_p_high), size=0, alpha=0.5) + 
  geom_point(data = data_by_subject, stat='summary', fun.y='mean') + 
  geom_linerange(data=data_by_subject, stat='summary', fun.data='mean_cl_boot') + 
  facet_grid(.~bvotCond) +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Probability /p/ response') + 
  scale_color_discrete('/b/ mean\nVOT') + 
  scale_fill_discrete('/b/ mean\nVOT')

@ 
\caption{The classification functions (shaded ribbons, 95\% posterior predictive
  intervals) predicted by the belief updating model fit listeners' responses
  well (dots with lines showing bootstrapped 95\% confidence intervals).}
\label{fig:model-fit-classifications}
\end{figure*}

The first way we evaluate this model is to ask how well it fits listeners'
behavior. Figure~\ref{fig:model-fit-classifications} shows listeners' average
classification functions, compared with the posterior predictive classification
functions from the belief-updating model.  The first thing to notice is that the
model fits the data well (log-likelihood ratio vs. an intercept-only binomial
null model of \Sexpr{round(mod_goodness_of_fit[['LL_ratio']])},
and Spearman's
$\rho$ = \Sexpr{mod_goodness_of_fit[['rho']]}, $p<10^{-10}$ in both cases) capturing the different
classification functions that result from exposure to each input distribution.
This in and of itself is an interesting result: it shows that there does exist
some set of prior beliefs such that the range of adaptation behavior we observed
can be explained by a model where the listeners assigned to the different accent conditions 
all start from a common set of prior beliefs. 

\begin{figure}[htb]
  \centering
<<inferred-prior, fig.width=4, fig.height=2, out.width='\\columnwidth'>>=

prior_summary <- 
  prior_samples_df %>% 
  gather('stat', 'val', mu_0:sigma_0) %>% 
  unite(stat_cat, stat, category) %>% 
  select(-cat_num) %>% spread(stat_cat, val) %>%
  gather('stat', 'value', kappa_0:sigma_0_p) %>% 
  group_by(stat) %>%
  summarise(mean=mean(value), 
            low=quantile(value, 0.025), 
            high=quantile(value, 0.975)) %>%
  mutate(units = ifelse(str_detect(stat, '(kappa|nu)'), 
                        'observations', 
                        'ms VOT'))

prior_expected <- prior_summary$mean
names(prior_expected) <- prior_summary$stat

prior_samples_df %>% 
  group_by(category) %>% 
  summarise(mean = mean(mu_0), sd = mean(sigma_0)) %>%
  stats_to_lhood(noise_sd = 0) %>%
  ggplot(aes(x=vot, y=lhood, group=category)) +
  geom_line(aes(linetype='Inferred\nprior')) +
  geom_line(data=prior_lhood, aes(linetype='Kronrod et\nal. (2012)')) +
  scale_linetype_discrete('Source') +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Likelihood')

@ 
  
\caption{Expected cue distributions based on the prior beliefs inferred here from
  behavioral adaptation data (solid lines). Plotted with VOT distributions measured by
  \protect\citeA{Kronrod2012} based on a combination of classification and
  discrimination behavior (dashed lines).}
  \label{fig:inferred-prior}
\end{figure}

\begin{table}[tb]
  \centering
<<inferred-prior-params, results='asis'>>=

format_params <- function(s) {
  s %>%
    str_replace('_0_([bp])$', '_{0,\\\\mathrm{\\1}}') %>%
    str_c('$\\', ., '$')
}

prior_summary %>% 
  transmute(Parameter = format_params(stat),
            Expected = mean,
            `95\\% HPD Int.` = sprintf('%.0f--%.0f', low, high),
            Units = units) %>%
  as.data.frame() %>%
  knitr::kable(format='latex', escape=FALSE)

@ 
  \caption{Expected values and 95\% highest posterior density intervals for the
    prior parameters, given the adaptation data.}
  \label{tab:prior-params}
\end{table}

<<rounding>>=
options(digits=0)
@ 

The second way to evaluate this model is based on the prior beliefs it infers
listeners to have.  Table~\ref{tab:prior-params} shows the posterior expectation
and 95\% highest posterior density intervals for each of the prior belief
parameters given the adaptation data above.  The behavioral data was consistent
with high confidence in prior beliefs with the prior confidence about category
variances ($E(\nu_0) = \Sexpr{prior_expected['nu_0']}$) higher than confidence
in the category means ($E(\kappa_0) = \Sexpr{prior_expected['kappa_0']}$).  Both
of these (measured in pseudo-observations) are larger than the number of trials
that listeners heard in the experiment (222), which means that as far as the
belief-updating model is concerned, listeners updated beliefs reflected their
prior beliefs as much as (in the case of the means) or more than (for the
variances) the distributions they actually observed.  This is consistent with
the qualitative finding that listeners' category boundaries are intermediate
between the boundaries corresponding to a typical talker and the experimental
exposure talker.

Figure~\ref{fig:inferred-prior} shows the cue distributions corresponding to the
posterior expected values of the prior expected mean and variance parameters
given the behavioral data, compared with the distributions corresponding to one
typical talker (as determined by a combination of classification and
discrimination data, \citeNP{Kronrod2012}, see also \citeNP{Lisker1964}).  The
inferred prior beliefs are in reasonably good agreement with this typical
talker, with one exception: the mean for /b/ is slightly lower
($E(\mu_{0,\mathrm{b}}) = \Sexpr{prior_expected['mu_0_b']}$ ms), and the
standard deviation of /b/ is slightly higher
($E(\sigma_{0,\mathrm{b}}) = \Sexpr{prior_expected['sigma_0_b']}$ ms).

\subsection{Discussion}
\label{sec:discussion-1}

These modeling results show that the pattern of adaptation behavior observed
above is consistent with a belief-updating model of phonetic adaptation that
combines prior expectations with input statistics in order to infer the current
talker's cue distributions.  Specifically, it shows that there exists a single
set of prior beliefs that captures the range of adaptation to different input
distributions that stretches from nearly complete adaptation to partial
adaptation at best.

These prior beliefs are reasonably consistent with other attempts to determine
what listeners think the underlying cue distributions are \cite{Kronrod2012}, as
well as the distributions produced by actual talkers \cite{Lisker1964}.  In
fact, the prior expected VOT distribution for /p/ that our model inferred is
almost identical to that observed by both \citeA{Kronrod2012} and
\citeA{Lisker1964}.  The distribution for /b/ deviates from prior work,
however. One possible reason for this is that a substantial minority of English
speakers produce pre-voiced /b/ \cite{Lisker1964}, which is
characterized by a lower (negative) VOT and a higher variance (often higher even
than /p/).  That is, across talkers, the /b/ VOT distribution parameters (mean
and variance) have a \emph{bimodal} distribution.  We assumed a single, unimodal
prior distribution, and the prior beliefs we inferred to be most likely are
consistent with a compromise between the two types of /b/ distributions that
talkers actually produce.
%% [TFJ: ok, but why did previous works not come to the same conclusion? in particular
%% kronrod? I think you need to say something about that.]

This possibility suggests two directions for future work. First, large-scale
corpus studies of VOT distributions are needed to determine to what extent the distribution of /b/ mean VOTs is really bimodal across talkers.  However, 
the only study
of this type we are aware of considers only non-negative VOTs
\cite{Chodroff2015} and thus excludes talkers who pre-voice their /b/.  Second,
more modeling work is needed to test whether a multimodal prior is justified
given the adaptation data, and if so, whether it would change the inference
about listeners' prior expectations for /b/.

The other major result of this modeling is that listeners have high confidence
in their prior expectations about the VOT distributions of /b/ and /p/, acting
as if they had already observed around 200--800 samples from each category (for
the category means and variances, respectively) from the unfamiliar talker they
encountered in our experiment.  

At first blush, this conflicts with previous work on another phonetic contrast,
/b/-/d/, which found confidence values that were one or two orders of magnitude
smaller than those inferred here \cite{Kleinschmidt2015}.  Interestingly, the
/b/-/d/ contrast is cued by spectral cues (formant frequency transitions) which
generally vary substantially across talkers \cite<e.g.,>{Peterson1952}.  The
acoustic cues to the /b/-/p/ contrast used in the current study do not show as
much variability across talkers \cite<e.g.,>{Allen2003, Chodroff2015}.  When
there is little variability across talkers, past experience with other talkers'
VOT distributions is highly informative about the distributions that an
unfamiliar talker will produce, requiring less adaptation.  Likewise, when there
is more variability across talkers, listeners need to rely more on the current
talker's cue distributions and less on their prior experience.  Thus, the
apparent discrepancy between the confidence that listeners place in their prior
beliefs in the current study and in \citeA{Kleinschmidt2015} is actually in line
with an \emph{ideal adapter} which combines prior beliefs with current
experience weighted according to confidence.  This idea finds further empirical
support in \citeA{Kraljic2007}, who found that after the same amount of
exposure, listeners recalibrate a /d/-/t/ contrast (analogous to the /b/-/p/
contrast used here) much less than an /s/-\ph S contrast (where the latter
exhibits larger variability across talkers; e.g., \citeNP{Newman2001}).

\section{Conclusion}
\label{sec:conclusion}

A central prediction of the ideal adapter framework \cite{Kleinschmidt2015} is
that listeners adapt to unfamiliar talkers by combining their prior beliefs with
observed evidence about that particular talker's cue distributions.  In this
paper, we have shown first that for a range of different accents (cue
distributions), listeners' behavior in a distributional learning experiment
reflects a compromise between what would be expected for the cue distributions
produced by a typical talker and the exposure talker.  Second, the range of
adaptation behavior observed across the various accents that listeners heard can
be captured by a belief-updating model with a single set of prior expectations
that are updated based on experience with the exposure talker.

These results emphasize the importance of listeners' prior expectations for
robust speech perception in the face of talker variability.  Even if all the
listener knows about the talker is that they are speaking English, they can
still benefit from prior experience with other speakers of English to provide an
informative head start for adaptation.  The modeling framework we use has the
additional advantage of allowing us to \emph{infer} what cue distributions
listeners believe an unfamiliar talker will produce. This provides a potentially
powerful---and heretofore missing---tool for probing listeners' prior
expectations, based only on comprehension data. These beliefs reflect what
listeners have learned about the variability they can expect across talkers, and
probing how this internal model is related to the \emph{actual} variability
across talkers (measured via speech production data) is an important next step
in advancing our understanding of robust speech perception.

More generally, prior knowledge is increasingly understood to play in important
role in a number of perceptual and memory domains
\cite<e.g.,>{Brady2013,Froyen2015a,Orhan2011}.  Distributional learning provides
an approach to probing prior expectations about the \emph{statistics} of the
sensory world, which, as in speech perception, are critical to effectively
coping with non-stationarity in sensory statistics.

\section{Acknowledgements}
\label{sec:acknowledgements}

This work was partially funded by an NSF Graduate Research Fellowship to DFK and
NICHD R01 HD075797 as well as an Alfred P. Sloan Fellowship to TFJ. The views
expressed here are those of the authors and not necessarily those of the funding
agencies.

\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{/Users/dkleinschmidt/Documents/papers/library-clean}



\end{document}
